# -*- coding: utf-8 -*-
"""S2_CAH_DU_SOUNALATH.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cob1D8_3MjdyPXzW1jF4QLLHZIJCKbM0

## **Projet Python et Algo** : Classification ascendante hiérarchique

DU Xinyi et SOUNALATH Alissa

Téléchargement de la stoplist française pour l'utiliser en donnée de test.
"""

!wget -q --show-progress --no-check-certificate "https://www.dropbox.com/scl/fi/vjogbpimtj0wck3fejxnf/stoplist.txt?rlkey=2l7kgyv54dy3x4o6xbv652jpl&dl=0" -O "stoplist.txt"

"""Téléchargement de fichiers texte pour les utiliser en données de test."""

!wget -q --show-progress --no-check-certificate "https://www.dropbox.com/scl/fi/ob79mhl118qczmx44zsqr/texte1.txt?rlkey=8s7604yxxic4tz4fmyo67yxvi&dl=0" -O "texte1.txt"

!wget -q --show-progress --no-check-certificate "https://www.dropbox.com/scl/fi/akwysc8s23za2vwdxd3pl/texte2.txt?rlkey=i8pbueqv5cwgce8jpm362nq9c&dl=0" -O "texte2.txt"

!wget -q --show-progress --no-check-certificate "https://www.dropbox.com/scl/fi/apfh55pf3y97435df4wwb/texte3.txt?rlkey=wopng9q728nz9fi1891rhadj0&dl=0" -O "texte3.txt"

!wget -q --show-progress --no-check-certificate "https://www.dropbox.com/scl/fi/sjeuruo2kwupu9r36qs53/texte4.txt?rlkey=o6zu71t6ar7g2hki3o6judjx3&dl=0" -O "texte4.txt"

"""##**Algorithme CAH**"""

#Importation des modules
import requests
from bs4 import BeautifulSoup
import string
import re
import math

def read_stoplist(filename: str) -> set:
    """
    Fonction qui lit la stoplist depuis un fichier et retourne un ensemble de mots
    Entrées:
        filename: le chemin vers le fichier contenant la stoplist (str)
    Sorties:
        Un ensemble de mots de la stoplist (set)
    """
    #on crée un ensemble vide utilisé pour stocker les mots de la stoplist
    stoplist = set()
    try:
        #ouvre le fichier
        with open(filename, 'r', encoding='utf-8') as file:
            #parcourt les lignes fichier
            for line in file:
                #supprime les espaces vides autour du mot et stocker dans word
                word = line.strip()
                #on vérifie que le mot n'est pas vide
                if word != "":
                    stoplist.add(word)
        return stoplist
    #si on trouve pas ou arrive pas à ouvrir le fichier renvoie une erreur
    except FileNotFoundError:
        print(f"Erreur : Fichier introuvable à l'emplacement '{filename}'.")
    except Exception as e:
        print(f"Erreur lors de la lecture du fichier '{filename}: {str(e)}")

def import_text(text_type: int, source: str) -> str:
    """
    Fonction qui permet d'importer le texte à partir de différentes sources.
    Entrées:
        text_type: indique le type de texte à importer (1 pour URL, 2 pour fichier, 3 pour chaîne de caractères)
        source: l'URL, le chemin du fichier ou la chaîne de caractères du texte
    Sortie:
        Le texte importé, ou une chaîne vide en cas d'erreur ou de texte vide
    """
    # Texte provenant d'une URL
    if text_type == "1":
        url = source
        try:
            # téléchargement du contenu HTML à une certaine URL
            response = requests.get(url)
            # analyse de la structure avec BS
            parsed = BeautifulSoup(response.text, "html.parser")
            # extraction du texte correspondant à <div class="text"></div>
            text = parsed.find_all('div', class_='text')
            if len(text):
                return text[0].text
            else:
                print("Erreur : Aucun texte trouvé à l'URL spécifiée.")
                return ""
        except Exception as e:
            print(f"Erreur lors de la requête HTTP vers l'URL '{url}': {str(e)}")
    # Texte provenant d'un fichier
    elif text_type == "2":
        filepath = source
        try:
            #ouvrir le fichier
            with open(filepath, 'r', encoding='utf-8') as file:
                text = file.read() #lire le fichier
            return text
        #si ne marche pas retourne une chaîne vide
        except FileNotFoundError:
            print(f"Erreur : Fichier introuvable à l'emplacement '{filepath}'.")
            return ""
        except Exception as e:
            return ""
    # Texte en tant que chaîne de caractères
    elif text_type == "3":
        text = source
        if text == "":
            print("Erreur : Texte vide.")
            return ""
        else:
            return text
    else:
        print("Erreur : Type de texte non valide.")
        return ""

def tokenize(text: str) -> list:
    """
    Tokeniseur
    Entrées:
        text : la chaîne à tokéniser
    Sorties:
        la liste des tokens
    """
    # Expression régulière et compilation
    #abréviations
    # Mots avec apostrophe
    tokgrm = re.compile(r"""
        (?:etc\.|p\.ex\.|cf\.|M\.|i\.e\.|dr\.|av\.|c\.-à-d\.)|
        \S+(?=(?:-(?:je|tu|ils?|elles?|nous|vous|leur|lui|les?|ce|t-|même|ci|là)))|
        [\w\-]+'?| # peut-être
        \b\w+(?:'\w+)?\b
        [^ ]
    """, re.X)
    tokens = tokgrm.findall(text)
    return tokens



class CAH:
    """Classe représentant l'algorithme de classification ascendante hiérarchique
    Attributes:
        _default_encoding (str): L'encodage utilisé

        Propriétés :
        - stoplist (set): Ensemble de mots-outils chargé depuis un fichier.
        - data (list): Liste permettant de stocker les données importées et traitées

        Méthodes statiques :
        - text2vec(tokens, stoplist): Compte la fréquence d'occurrences de chaque token, en évitant les mots de la stoplist
        - find_min_sim(dissims_list): Retourne la dissimilarité minimum dans une liste de dissimilarités
        - _dissim(cluster1, cluster2): Calcule la dissimilarité entre deux clusters de vecteurs
        - sim_cosinus(vect1, vect2): Calcule la similarité cosinus

        Instance:
        - __init__(stoplist_file): Initialise une instance de CAH en chargeant la stoplist à partir d'un fichier
        - add_text(text_list): Ajoute des textes provenant de différentes sources à data
        - del_text(label_to_delete): Supprime un texte de la liste de data
        - classify(n, min_sim):algo de classification avec n clusters et un seuil de similarité minimum défini par l'utilisateur
    """
    _default_encoding = 'utf8'

    def __init__(self, stoplist_file):
        """Initialise une instance de CAH.
        Args:
            stoplist_file (str): Le chemin vers le fichier contenant les mots outils
            data (list): Liste permettant de stocker les données importées et traitées

        """
        self.stoplist = read_stoplist(stoplist_file) #chargement stoplist à partir de stoplist_file
        self.data = [] #initialisation liste de donnés vides

    #Accesseur
    def get_data(self):
        return self.data

     #Méthodes d'instances
    def add_text(self, text_list: list):
            """
            Méthode qui ajoute des textes provenant de différentes sources à data.
            Entrées:
                text_list: une liste contenant des tuples de la forme (label, text_type, source)
                          où label est le label du texte, text_type est le type de texte
                          (1 pour URL, 2 pour fichier, 3 pour chaîne de caractères),
                          et source est l'URL, le chemin du fichier ou la chaîne de caractères.
            Sortie:
                La liste des données data mise à jour
            """
            #on parcourt chaque item dans la text_list
            for item in text_list:
                label_ask, text_type, source = item
                # Vérifier l'unicité du label
                #on initialise l'existence du label comme false par défaut
                label_exists = False
                #on parcourt chaque elt dans data
                for entry in self.data:
                    #si le label est trouvé
                    if entry["label"] == label_ask:
                        label_exists = True
                        break #on quitte la boucle
                # Si le label existe, afficher un message d'erreur et passer à l'itération suivante
                if label_exists:
                    print(f"Erreur : Le label '{label_ask}' existe déjà.")
                    continue #pour ne pas ajouter le texte si le label existe
                else:
                    # Importer le texte à partir de la source
                    text = import_text(text_type, source)
                    if text:
                        tokens = tokenize(text)
                        #effectuer la transformation en vecteur, avec les tokens filtrés
                        freq = self.text2vec(tokens, self.stoplist)
                        # Ajouter le texte avec son label et son vecteur à data
                        self.data.append({"label": label_ask, "vecteur": freq})
                        print(f"Le texte '{label_ask}' a été ajouté.")
                    else:
                        print(f"Erreur lors de l'importation du texte pour le label '{label_ask}'.")

    def del_text(self, label_to_delete: str):
        """
        Fonction qui permet de supprimer un texte de data
        Entrée :
          label_to_delete : le label du texte à supprimer (str)
        """
        try:
            # on initialise une variable pour savoir si le label a été trouvé
            found = False
            for item in self.data: #parcours de data
                #si le label de elt actuel == label_to_delete
                if item["label"] == label_to_delete:
                    #on supprime l'élément
                    self.data.remove(item)
                    #label trouvé
                    found = True
                    print(f"Le texte avec le label '{label_to_delete}' a été supprimé.")
                    break #on sort de la boucle
            if not found:
                print(f"Erreur : Aucun texte trouvé avec le label '{label_to_delete}'.")
        except Exception as e:
                print(f"Erreur lors de la suppression du texte avec le label '{label_to_delete}': {str(e)}")

    def classify(self, n: int, min_sim: float) -> list:
        """
        Fonction qui effectue la classification ascendante hiérarchique en s'arrêtant à n classes
        ou à une certaine similarité min_sim.

        Entrées:
            self: L'instance de la classe CAH.
            n (int): Le nombre de clusters souhaité.
            min_sim (float): Le seuil minimum de similarité pour fusionner les clusters.

        Sortie:
            list: La liste des clusters obtenus.
        """
        # on initialise les clusters
        clusters = []
        labels = []

        #on extrait les labels et vecteurs existants
        #parcours de data
        for entry in self.data:
            # ajouter le vecteur à clusters
            clusters.append([entry["vecteur"]])
            # ajouter le label à la liste de labels
            labels.append([entry["label"]])

        # répéter jusqu'à ce qu'il ne reste n clusters
        while len(clusters) > n:
            #on initialise min_dissim avec la valeur maximale pour continuer à fusionner
            min_dissim = 1
            merge_indices = None #on initialise une variable qui stockera des tuples d'indices

            # boucle imbriquées pour parcourir chaque clusters dans la liste de clusters
            for i in range(len(clusters)):
                #parcours les clusters restants à partir de i+1 pour comparer
                for j in range(i + 1, len(clusters)):
                    dissim = self._dissim(clusters[i], clusters[j])  #Calcul dissim
                    if dissim < min_dissim: # si la dissim est inférieure à la dissim minim actuelle
                        min_dissim = dissim #màj de la dissim minim
                        merge_indices = (i, j) #mettre à jour les indices

            if min_dissim < min_sim: #si dissim minimum est inférieure à min_sim spécifiée
                i, j = merge_indices
                merged_cluster = clusters[i] + clusters[j] # fusionner les vecteurs des clusters
                # trouver l'index maximum et minimum entre i et j
                if i > j:
                    max_index = i
                    min_index = j
                else:
                    max_index = j
                    min_index = i
                #supprimer les index minimum et max pour garantir la bonne suppression des clusters
                clusters.pop(max_index)
                clusters.pop(min_index)

                clusters.append(merged_cluster)
                # Mettre à jour les labels correspondants
                merged_label = labels[i] + labels[j]
                labels.pop(max(i, j))
                labels.pop(min(i, j))
                labels.append(merged_label)
            else:
                break  # Sortir de la boucle si la dissimilarité minimale dépasse le seuil
        return labels


    #Méthodes statique
    @staticmethod
    def text2vec(tokens, stoplist) -> dict:
        """
        Méthode qui compte la fréquence d'occurrences de chaque tokens
        en évitant les mots de la stoplist.
        Entrées:
            tokens: liste de tokens (list)
            stoplist: ensemble de mots-outils (set)
        Sorties:
            Dictionnaire de fréquences
        """

        freq = {}
        # Parcourt de chaque token dans la liste de tokens
        for token in tokens:
            #on normalise le texte et prend que les tokens qui ne sont pas dans la stoplist
            # si le token n'est pas dans la stoplist, mettre à jour sa fréquence
            if token.lower() not in stoplist:
                try:
                    freq[token] += 1
                # sinon on l'ajoute avec une fréquence de 1
                except KeyError:
                    freq[token] = 1
        return freq

    @staticmethod
    def find_min_sim(dissims_list) -> float:
        """
        Fonction qui lit la liste de dissimilarités et retourne la dissimilarité minimum
        Entrées:
          dissims_list: la liste de tous les résultats de dissimilarité entre tous les clusters
        Sorties:
          min_sim: float (la dissimilarité minimum entre deux clusters)
        """

        #on prend le premier élément de dissim_list dans min_sim
        min_sim = dissims_list[0]
        #on parcourt la liste dissims_lists
        for dissim in dissims_list:
          #si on trouve en dessous, dissim devient le minimum
          if dissim < min_sim:
            min_sim = dissim
        return min_sim


    @staticmethod
    def sim_cosinus(vect1: dict, vect2: dict) -> float:
        """
        Fonction qui calcule la similarité cosinus entre deux hachages (vecteurs) et trouver le cosinus minimum.
        Entrée :
            vect1 (dict) : Le premier vecteur sous forme de dictionnaire
            vect2 (dict) : Le deuxième vecteur sous forme de dictionnaire
        Sortie :
            float : La similarité cosinus entre les deux hachages
        """
        #on initialise à 0
        proscalaire = 0
        norme1 = 0
        norme2 = 0

        #parcourir toutes les comparaison entre les deux vects
        #calcul de cosinus
        for cle in vect1:
          if cle in vect2:
            proscalaire += vect1[cle] * vect2[cle]
            norme1 += vect1[cle] ** 2
            norme2 += vect2[cle] ** 2
        # Calculer les normes
        normes = math.sqrt(norme1) * math.sqrt(norme2)
        # Pour éviter une division par zéro
        if normes == 0:
          return 0
        else:
          cosinus = proscalaire / normes
          return cosinus

    @staticmethod
    def _dissim(cluster1: list, cluster2: list) -> float:
        """
        Méthode privée avec une fonction qui calcule la dissimilarité entre cluster1 et cluster2.

        Entrée :
            vect (dict) : Le vecteur sous forme de dictionnaire.
            cluster (list) : Le cluster de vecteurs sous forme de liste de dictionnaires.

        Sortie :
            float : La dissimilarité entre le vecteur et le cluster.
        """
        #initialise la dissimilarité totale à 0
        total_dissimilarity = 0
        #on parcourt chaque vecteur dans le premier cluster
        for cluster_vect1 in cluster1:
          #on parcourt chaque vecteur dans le deuxième cluster
          for cluster_vect2 in cluster2:
              # calcul de la dissimilarité entre le vecteur et chaque vecteur dans le cluster
              dissimilarity = 1 - CAH.sim_cosinus(cluster_vect1, cluster_vect2)
              # Mettre à jour la dissimilarité
              total_dissimilarity += dissimilarity

        # Faire la moyenne en divisant par le nombre de vecteurs dans le cluster
        dissimilarity = total_dissimilarity / (len(cluster1)*len(cluster2))
        return dissimilarity


# Utilisation
stoplist_file = input("Chemin vers le fichier de stoplist: ") #/content/stoplist.txt

#Instance objet cah
cah = CAH(stoplist_file)

# Menu de navigation
navigation = True
while navigation:
    print("Choississez l'option")
    #demander à l'utilisateur son choix
    choice = input("1: Ajouter un texte 2: Supprimer un texte 3: Afficher data 4: Quitter et lancer l'algorithme de classification ")
    #AJOUTER UN TEXTE
    if choice == "1":
        #on récupère text_list
        text_list = input("Entrez une liste de textes au format [(label1, type1, source1), (label2, type2, source2), ...] : ")
        try:
            #convertir
            text_list = eval(text_list)
            cah.add_text(text_list)
        #si impossible, renvoyer un message d'erreur
        except Exception as e:
            print("Erreur lors de l'ajout des textes :", str(e))
    #SUPPRIMER UN TEXTE
    elif choice == "2":
        label_to_delete = input("Entrez le label du texte à supprimer : \n")
        cah.del_text(label_to_delete)
    #AFFICHER DATA
    elif choice == "3":
        data = cah.get_data()
        if data: #vérifier que ce n'est pas vide
            print("Données actuelles :")
            #parcourir data
            for entry in data:
                print(f"Label: {entry['label']}, Vecteur: {entry['vecteur']}")
        else:
            print("Aucune donnée disponible.")
    #EXECUTER L'ALGO
    elif choice == "4":
            n = int(input("Veuillez entrez un nombre de clusters n: "))
            min_sim = float(input("Veuillez entrer le seuil minimum de similarité: "))
            print("Exécution de l'algorithme")
            #Execution de l'algo avec les paramètres spécifiés par l'utilisateur
            result = cah.classify(n, min_sim)
            print("Clusters finaux obtenus:")
            for label in result:
                print(label)
            navigation = False
            break #quitter le menu de navigation
    else:
        print("Choix invalide")

"""#Version 2 classify : version simplifiée"""

# @title
import requests
from bs4 import BeautifulSoup
import string
import math
import copy
import re

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage


def read_stoplist(filename: str) -> set:
  """
  Fonction qui lit la stoplist depuis un fichier et retourne un ensemble de mots.
  Entrées:
      filename: le chemin vers le fichier contenant la stoplist
  Sorties:
      Un ensemble de mots de la stoplist
  """
  stoplist = set()
  with open(filename, 'r', encoding='utf-8') as file:
      for line in file:
          word = line.strip()
          if word != "":
              stoplist.add(word)
  return stoplist


def import_text_from_url(url: str) -> str:
    """
    Fonction qui importe le texte à partir d'une URL.
    Entrées:
        url: l'URL à partir de laquelle importer le texte
    Sorties:
        Le texte importé
    """
    # téléchargement du contenu HTML à une certaine URL
    response = requests.get(url)
    # analyse de la structure avec BS
    parsed = BeautifulSoup(response.text, "html.parser")
    # extraction du texte correspondant à <div class="text"></div>
    text = parsed.find_all('div', class_='text')
    if len(text):
        return text[0].text
    else:
        return ""


def tokenize(txt): #revoir la tokénisation
    """
    Tokeniseur
    Entrées:
        text : la chaîne à tokéniser
    Sorties:
        la liste des tokens
    """
    txt = txt.lower()
    tokgrm=re.compile(r"""
        (?:etc.|p.ex.|cf.|M.|i.e.|dr.|av.|c.-à-d.)|
        \S+(?=(?:-(?:je|tu|ils?|elles?|nous|vous|leur|lui|les?|ce|t-|même|ci|là)))|
        [\w\-]+'?| # peut-être
        [^ ]
    """,re.X)
    tokens = tokgrm.findall(txt)
    return tokens


def import_text(text_type:int, input_text) -> str:
    """
    Méthode pour ajouter un texte provenant de plusieurs sources et alimenter data :
          - à partir d'une URL
          - à partir d'un fichier
          - à partir d'une chaîne de caractères
      Entrées:
          text_type: le type de texte à ajouter (1 pour URL, 2 pour fichier, 3 pour chaîne de caractères)
      Sortie:
          text: chaine de caractères
    """

    ### Traiter le texte en fonction de son type
    # Texte provenant d'une URL
    if text_type == 1:
      url = input_text
      try:
        text = import_text_from_url(url)
        if text:
            return text
        else:
            print(f"Erreur : Impossible d'importer le texte à partir de l'URL '{url}'. Le texte est vide.")
      except Exception as e:
          print(f"Erreur lors de l'importation du texte à partir de l'URL '{url}': {str(e)}")

    # Texte provenant d'un fichier
    elif text_type == 2:
      filepath = input_text
      try:
        with open(filepath, 'r', encoding='utf-8') as file:
          text = file.read()
          if text:
            return text
          else:
              print("Erreur : Le fichier est vide.")
      except FileNotFoundError:
          print("Erreur : Fichier introuvable.")

    # Texte en tant que chaîne de caractères
    elif text_type == 3:
      text = input_text
      if text != "":
        return text
      else:
        print("Erreur : Texte vide.")

    # ne pas pouvoir detecter le type de texte
    else:
        print("Erreur : Type de texte non valide.")


def find_min_sim(dissims_list):
    """
    Fonction qui lit la liste de dissimilarités et retourne la dissimilarité minimume
    Entrées:
      dissims_list: la liste de tous les résultats de dissimilarité entre tous les clusters
    Sorties:
      min_sim: float (la dissimilarité minimume entre deux clusters)
    """
    #initialiser min_dissim
    min_dissim = dissims_list[0]
    #parcourir tous les resultat de dissimilarites
    for dissim in dissims_list:
      if dissim < min_dissim:
        min_dissim = dissim # prendre le resultat le plus bas

    return min_dissim



class CAH:
  """
  Classe représentant l'algorithme de classification ascendante hiérarchique
    Attributes:
        _default_encoding (str): L'encodage utilisé

        Propriétés :
        - stoplist (set): Ensemble de mots-outils chargé depuis un fichier.
        - data (list): Liste permettant de stocker les données importées et traitées

        Méthodes statiques :
        - text2vec(tokens, stoplist): Compte la fréquence d'occurrences de chaque token, en évitant les mots de la stoplist
        - _dissim(cluster1, cluster2): Calcule la dissimilarité entre deux clusters de vecteurs
        - sim_cosinus(vect1, vect2): Calcule la similarité cosinus

        Instance:
        - __init__(stoplist_file): Initialise une instance de CAH en chargeant la stoplist à partir d'un fichier
        - add_text(text_list): Ajoute des textes provenant de différentes sources à data
        - del_text(label_to_delete): Supprime un texte de la liste de data
        - classify(n, min_sim):algo de classification avec n clusters
        - tf_idf(): algo qui calcule tous les mots apparus dans des textes importes
  """
  _default_encoding = 'utf8'

  ###Initialisation de l'instance###
  def __init__(self, stoplist_file):
      self.stoplist = read_stoplist(stoplist_file)
      self.data = []  # Liste permettant de stocker les données importées

  ###Méthode de l'instance###
  def add_text(self, type_text, label, input_text):
      """
      ajouter un texte provenant de plusieurs sources et alimenter data

      Entrées：
        type_text(int): le type de texte à ajouter (1 pour URL, 2 pour fichier, 3 pour chaîne de caractères)
        label(str): le titre de la texte ajoutée
        input_text(str): un texte provenant de plusieurs sources et alimenter data :
        - à partir d'une URL
        - à partir d'un fichier
        - à partir d'une chaîne de caractères
      Sorties:
        liste de data(list): liste permettant de stocker les données importées
      """

      text = import_text(type_text, input_text) #defini le type de texte importe 1) à partir d'une URL  2) à partir d'un fichier  3) à partir d'une chaîne de caractères et le input_text
      label_text = {} # initialiser le dictionnaire lie le label au texte importe
      label_text[label]= text
      self.data.append(label_text) #alimenter dictionnaire label_text dans self.data

      return self.data

  ###Méthode de l'instance###
  def del_text(self, label_to_delete):
      """
      Supprimer un texte de la liste en fonction de son label.

      Entrée:
        label_to_delete(str): label tapé
      Sortie:
        liste de data(list): liste permettant de stocker les données importées
      """

      try:
          found = False
          for item in self.data:
              if label_to_delete in item: # si la label a supprimer est dans le dictionnaire label_text
                  self.data.remove(item)
                  found = True
                  print(f"Le texte avec le label '{label_to_delete}' a été supprimé.")
                  break
          if not found:
              print(f"Erreur : Aucun texte trouvé avec le label '{label_to_delete}'.")
      except Exception as error:
          print(f"Erreur lors de la suppression du texte avec le label '{label_to_delete}': {str(error)}")


  #Méthode statique
  @classmethod
  def text_2_vec(cls, text, stoplist):
      """
      Fonction qui compte la fréquence d'occurrences de chaque tokens
      en évitant les mots de la stoplist.
      Entrées:
          tokens: liste de tokens
          stoplist: ensemble de mots-outils
      Sorties:
          Dictionnaire de fréquences
      """

      vecteur = {}
      tokens = tokenize(text)
      for token in tokens:
          #on normalise le texte et prend que les tokens qui ne sont pas dans la stoplist
          if token.lower() not in stoplist:
            try:
              vecteur[token] += 1
            except KeyError:
              vecteur[token] = 1
      return vecteur

  #Méthode statique
  @classmethod
  def sim_cosinus(cls, vect1, vect2):
      """
      Fonction qui calcule la similarité cosinus entre deux hachages (vecteurs) et trouver le minimun  cosumus.

      Entrée :
          vect1 (dict) : Le premier hachage (vecteur) sous forme de dictionnaire.
          vect2 (dict) : Le deuxième hachage (vecteur) sous forme de dictionnaire.

      Sortie :
          float : La similarité cosinus entre les deux hachages.
      """

      proscalaire = 0
      norme1 = 0
      norme2 = 0

      #parcourir toutes les comparaison entre les deux vects
      #calcul de cosinus
      for cle in vect1:
        if cle in vect2:
          proscalaire += vect1[cle] * vect2[cle]
          norme1 += vect1[cle] ** 2
          norme2 += vect2[cle] ** 2
      norme = math.sqrt(norme1) * math.sqrt(norme2)

      if norme == 0:
        return 0  # Pour éviter une division par zéro
      else:
        cosinus = proscalaire / norme
        return cosinus

      #enregistrer les resultats
      cos_list.append(cosinus)


  #Méthode statique
  @classmethod
  def calcul_dissim(cls, cluster1, cluster2):
      """
      Méthode privée avec une fonction qui calcule la dissimilarité entre cluster1 et cluster2.

      Entrée :
          vect (dict) : Le vecteur sous forme de dictionnaire.
          cluster (list) : Le cluster de vecteurs sous forme de liste de dictionnaires.

      Sortie :
          float : La dissimilarité entre le vecteur et le cluster.
      """
      total_dissimilarity = 0
      for cluster_vect1 in cluster1:
        for cluster_vect2 in cluster2:
            # Calculer la dissimilarité entre le vecteur et chaque vecteur dans le cluster
            dissimilarity = 1 - CAH.sim_cosinus(cluster_vect1, cluster_vect2)
            # Ajouter la dissimilarité au total
            total_dissimilarity += dissimilarity

      # Calculer la dissimilarité moyenne en divisant par le nombre de vecteurs dans le cluster
      dissimilarity = total_dissimilarity / (len(cluster1)*len(cluster2))

      return dissimilarity

  ### Méthode de l'instance ###
  def classify (self, n):
      """
      Fonction qui traite la liste stockant les données importées
        - détacher le texte de son label correspondant
        - exécuter text to vecteur
        - stoker les vecteur dans une nouvelle liste
        - parcourir tous les clusters et calculer la dissimilarité
        - trouver les deux clusters ayant la dissimilarité la plus base
        - fusionner ces deux clusters dans un nouveau cluster
        - supprimer les deux anciens clusters
      Entrée:
        n(int): le nombre de clusters formés à la fin
      Sortie:
        la liste de clusters
      """

      list_clusters = [] #liste permettant de stoker les vecteurs
      list_labels = [] #liste permettant de stoker les labels

      # parcourir tous les dictionnaire label_text dans self.data
      for item in self.data: #
        text = list(item.values())[0] #detacher chaque text de son label correspondant
        label = [list(item.keys())[0]]  #detacher a l'ordre le label corresspondant
        cluster = [CAH.text_2_vec(text, self.stoplist)] #transformer le text en un vecteur, puis, mettre le vecteur dans une liste pour le traiter comme un cluster
        list_clusters.append(cluster)
        list_labels.append(label)

      # le code s'arrete jusqu'au moment ou le nombre de clusters formes correspond au nombre designe de l'utilisateur
      while len(list_clusters) > n:
        dissim_dict = {} #initialiser le dictionnaire lie chaque dissimilarite de deux clusters a leurs indices
        dissims_list = [] #initilaiser la liste qui permet de stoker toues les dissimilarites entre deux clusters

        # Parcourir toutes les possibilités de comparaison entre deux clusters
        for i in range(len(list_clusters)):
          for j in range(i + 1, len(list_clusters)):
            dissim = CAH.calcul_dissim(list_clusters[i], list_clusters[j]) #claculer la dissimilarite de deux clusters
            dissim_dict[dissim] = i, j #stoker les indices de ces deux clusters
            dissims_list.append(dissim) #stiker le resultat de dissimilarite dans la liste

        min_dissim = min(dissims_list) #trouver la dissimilarité miniale dans tous les résultats
        indices = dissim_dict[min_dissim] #trouver les deux indices de deux clusters qui ont la dissimilarité miniamle

        # fusionner deux anciens clusters dans un nouvel cluster ainsi que leurs labels dans un nouvel label
        new_label = [copy.deepcopy(list_labels[index]) for index in indices]
        new_cluster = [copy.deepcopy(list_clusters[index]) for index in indices]

        # Supprimer les deux anciens labels
        del list_labels[max(indices)]
        del list_labels[min(indices)]

        # Supprimer les deux anciens clusters
        del list_clusters[max(indices)]
        del list_clusters[min(indices)]

        # Renouveler la liste de clusters
        list_clusters.append(new_cluster)
        list_labels.append(new_label)

      return list_labels

  ### méthode de l'instance ###
  def tf_idf(self) -> dict:
      """
      Calcule le score TF-IDF pour chaque mot dans les textes importés.
      Retour:
      dict: Un dictionnaire contenant le TF, IDF et TF-IDF pour chaque mot
      """
      # Dictionnaire pour stocker tf idf
      result = {}

      # Nb total de textes dans le corpus
      nb_total_txt = len(self.data)

      # dictionnaire pour stocker le nb de txt qui contiennent le mot
      nb_text_word = {}

      # Parcourir tous les textes dans self.data et récupérer le txt associé
      for item in self.data:
          text = list(item.values())[0]
          # transformatino du texte en vecteur
          vecteur = CAH.text_2_vec(text, self.stoplist)

          # Calcul du nb de mots totaux
          nb_total_word = sum(vecteur.values())

          # Parcours des mots dans le vecteur
          for word, freq in vecteur.items():
              # Calcul du tf
              tf = freq / nb_total_word

              # màj du nombre de textes contenant ce mot
              if word in nb_text_word:
                  nb_text_word[word] += 1
              else:
                  nb_text_word[word] = 1

              # Calcul de l'IDF pour ce mot
              idf = math.log(nb_total_txt / nb_text_word[word])

              # Calculer du TF IDF
              tf_idf = tf * idf

              # mettre le résultat dans le dictionnaire de résultats
              result[word] = {"TF": tf, "IDF": idf, "TF_IDF": tf_idf}

      return result

# @title
text1 = CAH("stoplist.txt")


text1.add_text(1, "chaperon rouge", 'https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Petit_Chaperon_rouge')
text1.add_text(1, "le petit poucet", 'https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Le_Petit_Poucet')
text1.add_text(1, "les Fees", 'https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Les_F%C3%A9es')
text1.add_text(2, "le mythe", '/content/texte1.txt')
text1.add_text(2, "sisphe",'/content/texte2.txt')

text1.classify(2)

# @title
text1.tf_idf()

"""# Dendogramme

Nous avons écrit un code mais pas réussi à implémenter la fonction
"""

# Méthode de l'instance
def dendrogram(self):
    list_vecteurs = []
    for item in self.data:
        text = list(item.values())[0]
        vecteur = CAH.text_2_vec(text, self.stoplist)
        list_vecteurs.append(vecteur)

    # Convertir la liste de listes en une matrice NumPy
    X = np.vstack(list_vecteurs)

    # Utiliser la méthode linkage de scipy pour effectuer le clustering hiérarchique
    Z = linkage(X, method='ward')

    # Afficher le dendrogramme
    plt.figure(figsize=(10, 5))
    plt.title('Dendrogramme de la classification ascendante hiérarchique')
    dendrogram(Z)
    plt.xlabel('Index des échantillons')
    plt.ylabel('Distance euclidienne')
    plt.show()

"""# Tests et résultats

Les résultats des tests concernent la version 1

Stoplist: /content/stoplist.txt

* **Test 1** :

Tester l'algo de classification avec des sources bien spécifiées

[("Chaperon", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Petit_Chaperon_rouge"),
("Barbe Bleue", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/La_Barbe_bleüe"),
("Belle", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/La_Belle_au_bois_dormant"),("Petit poucet", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Le_Petit_Poucet"), ("Fées", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Les_F%C3%A9es"), ("Mythologie", "2", "/content/texte1.txt"), ("Sisyphus","2", "/content/texte2.txt"),  ("Article IA", "2", "/content/texte3.txt"), ("Rousseau", "2", "/content/texte4.txt"), ("Marie", "3", "Helas ! je n’ay pour mon objet Qu’un regret, qu’une souvenance : La terre embrasse le sujet, En qui vivoit mon esperance. Cruel tombeau, je n’ay plus rien, Tu as dérobé tout mon bien, Ma mort, et ma vie, L’amant et l’amie, Plaints, souspirs, et pleurs, Douleurs sus douleurs. Que ne voy-je, pour languir mieux, Et pour vivre en plus longue peine, Mon cœur en souspirs, et mes yeux Se changer en une fonteine, Mon corps en voix se transformer, Pour souspirer, pleurer, mommer Ma mort, et ma vie, L’amant et l’amie, Plaints, souspirs, et pleurs, Douleurs sus douleurs. Ou je voudrais estre un rocher, Et avoir le cœur insensible, Ou esprit, afin de cercher Sous la terre mon impossible : J’irois sans crainte du trespas Redemander aux Dieux d’embas Ma mort, et ma vie. Mais ce ne sont que fictions : Il me fault trouver autres plaintes. Mes veritables passions Ne se peuvent servir de feintes. Le meilleur remede en cecy, C’est mon torment et mon soucy, Ma mort, et ma vie. Au pris de moy les amoureux Voyant les beaux yeux de leur dame, Cheveux et bouche, sont heureux De bruler d’une vive flame. En bien servant ils ont espoir : Je suis sans espoir de revoir Ma mort, et ma vie. Ils aiment un sujet qui vit : La beauté vive les vient prendre, L’œil qui voit, la bouche qui dit : Et moy je n’aime qu’une cendre. Le froid silence du tombeau Enferme mon bien, et mon beau, Ma mort, et ma vie. Ils ont le toucher et l’ouyr, Avant-courriers de la victoire : Et je ne puis jamais jouyr Sinon d’une triste memoire, D’un souvenir, et d’un regret, Qui tousjours lamenter me fait. Ma mort, et ma vie. L’homme peult gaigner par effort Mainte bataille, et mainte ville : Mais de pouvoir vaincre la Mort C’est une chose difficile. Le ciel qui n’a point de pitié, Cache sous terre ma moitié, Ma mort, et ma vie. Apres sa mort, je ne de vois Tué de douleur, la survivre : Autant que vive je l’aimois, Aussi tost je la de vois suivre : Et aux siens assemblant mes os, Un mesme cercueil eust enclos Ma mort, et ma vie. Je mettrais fin à mon malheur, Qui hors de raison me transporte, Si ce n’estoit que ma douleur D’un double bien me reconforte. La penser Déesse, et songer En elle, me fait allonger Ma mort, et ma vie. En songe la nuict je la voy Au ciel une estoille nouvelle S’apparoistre en esprit à moy Aussi vivante, et aussi belle Comme elle estoit le premier jour Qu’en ses beaux yeux je veis Amour, Ma mort, et ma vie. Sur mon lict je la sens voler, Et deviser de mille choses : Me permet le voir, le parler, Et luy baiser ses mains de roses : Torche mes larmes de sa main, Et presse mon cœur en son sein, Ma mort, et ma vie. La mesme beauté qu’elle avoit, La mesme Venus, et la grace, Le mesme Amour qui la suivoit, En terre apparoist en sa face, Fors que ses yeux sont plus ardans, Où plus à clair je voy dedans Ma mort, et ma vie. Elle a les mesmes beaux cheveux, Et le mesme trait de la bouche, Dont le doux ris, et les doux nœuds Eussent lié le plus farouche : Le mesme parler, qui souloit Mettre en doute, quand il vouloit Ma mort, et ma vie. Puis d’un beau jour qui point ne faut, Dont sa belle ame est allumée, Je la voy retourner là haut Dedans sa place accoustumée, Et semble aux anges deviser De ma peine, et favoriser Ma mort, et ma vie. Chanson, mais complainte d’amour, Qui rends de mon mal tesmoignage, Fuy la court, le monde, et le jour : Va-t’en dans quelque bois sauvage, Et de là ta dolente vois Annonce aux rochers, et aux bois Ma mort, et ma vie, L’amant et l’amie, Plaints, souspirs, et pleurs, Douleurs sus douleurs.")]

##Résultat

Avec n = 2, min_sim = 0.4
['Marie']
['Sisyphus', 'Rousseau']
['Petit poucet', 'Barbe Bleue', 'Belle', 'Mythologie', 'Chaperon', 'Fées', 'Article IA']
Renvoie 3 clusters

Avec n = 2, min_sim = 0.8
['Marie']
['Sisyphus', 'Rousseau', 'Petit poucet', 'Barbe Bleue', 'Belle', 'Mythologie', 'Chaperon', 'Fées', 'Article IA']

Avec n = 4, min_sim = 0.5
['Marie']
['Sisyphus', 'Rousseau']
['Petit poucet', 'Barbe Bleue', 'Belle']
['Mythologie', 'Chaperon', 'Fées', 'Article IA']

avec n = 4, min_sim = 0.2
['Petit poucet']
['Mythologie']
['Marie']
['Chaperon', 'Fées', 'Article IA']
['Sisyphus', 'Rousseau']
['Barbe Bleue', 'Belle']

Le code renvoie 6 clusters = seuil de similarité non atteint pour certains groupes?



* **Test 2 :**

Tester les messages d'erreurs lorsque la liste de texte à ajouter n'est pas bonne.

[("Chaperon", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Petit_Chaperon_rouge"), ("Chaperon", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Petit_Chaperon_rouge"), ("Barbe Bleue", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/La_Barbe_bleüe"), ("Belle", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/La_Belle_au_bois_dormant"),("Petit poucet", "0", "https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Le_Petit_Poucet"), ("Fées", "1", "https://fr.wikisource.org/wiki/Histoires_ou_Contemps_pass%C3%A9_(1697)/Les_F%C3%A9es"), ("Mythologie", "2", "/content/texte1.txt"), ("Sisyphus","2", "/content/texte2.txt"), ("Article IA", "2", "/content/e.txt"), ("Rousseau", "2", "/content/texte4.txt")]

## Résultat

* Le texte 'Chaperon' a été ajouté.
* Erreur : Le label 'Chaperon' existe déjà.
* Le texte 'Barbe Bleue' a été ajouté.
* Le texte 'Belle' a été ajouté.
* Erreur : Type de texte non valide.
* Erreur lors de l'importation du texte pour le label 'Petit poucet'.
* Erreur : Aucun texte trouvé à l'URL spécifiée.
* Erreur lors de l'importation du texte pour le label 'Fées'.
* Le texte 'Mythologie' a été ajouté.
* Le texte 'Sisyphus' a été ajouté.
* Erreur : Fichier introuvable à l'emplacement '/content/e.txt'.
* Erreur lors de l'importation du texte pour le label 'Article IA'.
* Le texte 'Rousseau' a été ajouté

# Données

Chemin de la stoplist, téléchargée précédemment : /content/stoplist.txt

URL:

* https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Petit_Chaperon_rouge
* https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/La_Barbe_ble%C3%BCe
* https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/La_Belle_au_bois_dormant
* https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/La_Barbe_bleüe
* https://fr.wikisource.org/wiki/Histoires_ou_Contes_du_temps_pass%C3%A9_(1697)/Le_Maistre_Chat


Chemin des fichiers:
* /content/texte1.txt
* /content/texte2.txt
* /content/texte3.txt
* /content/texte4.txt



STR:

"Helas ! je n’ay pour mon objet Qu’un regret, qu’une souvenance : La terre embrasse le sujet, En qui vivoit mon esperance. Cruel tombeau, je n’ay plus rien, Tu as dérobé tout mon bien, Ma mort, et ma vie, L’amant et l’amie, Plaints, souspirs, et pleurs, Douleurs sus douleurs. Que ne voy-je, pour languir mieux, Et pour vivre en plus longue peine, Mon cœur en souspirs, et mes yeux Se changer en une fonteine, Mon corps en voix se transformer, Pour souspirer, pleurer, mommer Ma mort, et ma vie, L’amant et l’amie, Plaints, souspirs, et pleurs, Douleurs sus douleurs. Ou je voudrais estre un rocher, Et avoir le cœur insensible, Ou esprit, afin de cercher Sous la terre mon impossible : J’irois sans crainte du trespas Redemander aux Dieux d’embas Ma mort, et ma vie. Mais ce ne sont que fictions : Il me fault trouver autres plaintes. Mes veritables passions Ne se peuvent servir de feintes. Le meilleur remede en cecy, C’est mon torment et mon soucy, Ma mort, et ma vie. Au pris de moy les amoureux Voyant les beaux yeux de leur dame, Cheveux et bouche, sont heureux De bruler d’une vive flame. En bien servant ils ont espoir : Je suis sans espoir de revoir Ma mort, et ma vie. Ils aiment un sujet qui vit : La beauté vive les vient prendre, L’œil qui voit, la bouche qui dit : Et moy je n’aime qu’une cendre. Le froid silence du tombeau Enferme mon bien, et mon beau, Ma mort, et ma vie. Ils ont le toucher et l’ouyr, Avant-courriers de la victoire : Et je ne puis jamais jouyr Sinon d’une triste memoire, D’un souvenir, et d’un regret, Qui tousjours lamenter me fait. Ma mort, et ma vie. L’homme peult gaigner par effort Mainte bataille, et mainte ville : Mais de pouvoir vaincre la Mort C’est une chose difficile. Le ciel qui n’a point de pitié, Cache sous terre ma moitié, Ma mort, et ma vie. Apres sa mort, je ne de vois Tué de douleur, la survivre : Autant que vive je l’aimois, Aussi tost je la de vois suivre : Et aux siens assemblant mes os, Un mesme cercueil eust enclos Ma mort, et ma vie. Je mettrais fin à mon malheur, Qui hors de raison me transporte, Si ce n’estoit que ma douleur D’un double bien me reconforte. La penser Déesse, et songer En elle, me fait allonger Ma mort, et ma vie. En songe la nuict je la voy Au ciel une estoille nouvelle S’apparoistre en esprit à moy Aussi vivante, et aussi belle Comme elle estoit le premier jour Qu’en ses beaux yeux je veis Amour, Ma mort, et ma vie. Sur mon lict je la sens voler, Et deviser de mille choses : Me permet le voir, le parler, Et luy baiser ses mains de roses : Torche mes larmes de sa main, Et presse mon cœur en son sein, Ma mort, et ma vie. La mesme beauté qu’elle avoit, La mesme Venus, et la grace, Le mesme Amour qui la suivoit, En terre apparoist en sa face, Fors que ses yeux sont plus ardans, Où plus à clair je voy dedans Ma mort, et ma vie. Elle a les mesmes beaux cheveux, Et le mesme trait de la bouche, Dont le doux ris, et les doux nœuds Eussent lié le plus farouche : Le mesme parler, qui souloit Mettre en doute, quand il vouloit Ma mort, et ma vie. Puis d’un beau jour qui point ne faut, Dont sa belle ame est allumée, Je la voy retourner là haut Dedans sa place accoustumée, Et semble aux anges deviser De ma peine, et favoriser Ma mort, et ma vie. Chanson, mais complainte d’amour, Qui rends de mon mal tesmoignage, Fuy la court, le monde, et le jour : Va-t’en dans quelque bois sauvage, Et de là ta dolente vois Annonce aux rochers, et aux bois Ma mort, et ma vie, L’amant et l’amie, Plaints, souspirs, et pleurs, Douleurs sus douleurs."

# Pseudo Code

##Fonction read_stoplist()



```
Fonction read_stoplist(string filename): string {
    Initialiser une set vide
    Try:
        Ouvrir le fichier filename en mode lecture
        Pour chaque ligne dans le fichier:
            word <- ligne.strip()
            Si word n'est pas une chaîne vide:
                Ajouter word à stoplist
        Retourner stoplist
    Except Erreur:
        afficher("erreur lors de l'ouverture")
}
```



##Fonction import_text()


```
Fonction import_text(entier type_texte): string {

    tant que True:
        si type_texte == "1":
            url <- input("saisissez l'url")
            Try:
                réponse <- envoyer une requête à url
                parsed <- analyser_réponse
                textes <- extraire_texte(parsed)
                si longueur(textes) > 0 alors:
                    retourner textes[0]
                sinon:
                    afficher("Texte non trouvé")
                    retourner ""
            Exception:
                afficher("Erreur lors de la requête http")

        sinon:
            si type_texte == "2":
            chemin_fichier <- input("saisissez le chemin du fichier ")
            essayer:
                ouvrir le fichier
                texte <- lire_fichier("fichier")
                retourner texte
            sauf FichiernonTrouvé:
                afficher("Erreur, fichier introuvable à l'emplacement spécifié")
                retourner ""
            sauf Exception:
                afficher("Erreur lors de la lecture du fichier")


        sinon:
            si type_texte ==  "3":
            texte <- input("saisissez str ")
            si texte == "":
                afficher("Erreur : Texte vide")
                retourner ""
            sinon:
                retourner texte
        sinon:
            afficher("Erreur : Type de texte non valide")
            retourner ""
}
```

##Tokenize()


```
Fonction tokenize(string texte): liste de chaînes{
        tokens = []
        #on utise une expression régulière complexe
        tokgrm <- re.compile(r'''
        #capturer les abréviations
        (?:etc\.|p\.ex\.|cf\.|M\.|i\.e\.|dr\.|av\.|c\.-à-d\.)|
        #les mots suivis d'un trait d'union suivi de pronoms
        \S+(?=(?:-(?:je|tu|ils?|elles?|nous|vous|leur|lui|les?|ce|t-|même|ci|là)))|
        [\w\-]+'?| # peut-être
         #mots entier qui commencent par une lettre - éviter les retour chariots lors de l'importation
        \b\w+(?:'\w+)?\b
        [^ ]
    ''', re.X)

    pour chaque token dans tokgrm.findall(text):
        tokens <- token

    retourner tokens

}
```





#**CAH**

##__init__()

```
Fonction __init__(string stoplist_file): {
    stoplist <- read_stoplist(stoplist_file)
    data <- []
}
```

##getdata()
```
Fonction get_data(self):
    retourner self.data
```

##add_text()

```
Fonction add_text(liste text_list) {
    pour chaque item dans text_list:
        label_ask, text_type, source <- item
        label_exists <- False
        pour chaque entry dans self.data:
            si entry["label"] == label_ask:
                label_exists <- True
                break
        si label_exists:
            afficher("Erreur : Le label existe déjà")
            continuer
        sinon:
            text <- import_text(text_type, source)
            si text != "":
                tokens <- tokenize(text)
                #transformation en vecteur
                freq <- self.text2vec(tokens, self.stoplist)
                self.data.append({"label": label_ask, "vecteur": freq})
                afficher("Le texte a été ajouté")
            sinon:
                afficher "Erreur lors de l'importation du texte"
}

```

##del_text()

Fonction del_text(self, string label_to_delete) {
```
    Try:
      Found <- False
      Pour chaque item dans self.data :
          Si item["label"] == label_to_delete:
              self.data.remove(item)
              Found <- True
              Afficher("Le texte avec le label '<label_to_delete>' a été supprimé")
              Quitter la boucle
      Si not found :
          Afficher("Erreur : Aucun texte trouvé avec le label '<label_to_delete>'")
    Except erreur:
          Afficher("Erreur lors de la suppression du texte avec le label '<label_to_delete>'")
}
```
##classify() + version 2 à la fin du document
```
Fonction classify(self, entier n, float min_sim): list {
    clusters = []
    labels = []

    #extraction des vecteurs et labels de data
    pour chaque entry dans self.data:
        clusters.append([entry["vecteur"]])
        labels.append([entry["label"]])
    

    tant que longueur de clusters > n:
        #on initialise min_dissim avec la valeur maximale pour continuer à fusionner
        min_dissim <- 1
        merge_indices <- None

        pour i allant de 0 à longueur de clusters - 1:
            pour j allant de i + 1 à longueur de clusters - 1:
                dissim <- self._dissim(clusters[i], clusters[j])
                si dissim < min_dissim:
                    min_dissim <- dissim
                    merge_indices <- (i, j)

        si min_dissim < min_sim:
            i, j <- merge_indices
            merged_cluster <- clusters[i] + clusters[j]
            si i > j alors
                max_index <- i
                min_index <- j
            sinon:
                max_index <- j
                min_index <- i
            clusters.pop(max_index)
            clusters.pop(min_index)

            clusters.append(merged_cluster)
            merged_label <- labels[i] + labels[j]
            labels.pop(max(i, j))
            labels.pop(min(i, j))
            labels.append(merged_label)
        sinon:
            Sortir de la boucle
    retourner labels
}
  


```
##text2vec()

```
Fonction text2vec(liste tokens, set stoplist) -> dict:{
    freq <- {}
    Pour chaque token.lower() dans tokens:
        Si tokens not in stoplist:
            Try:
                freq[token] += 1
            Except KeyError:
                freq[token] = 1
    Retourner freq
}
```
##find_min_sim()

```
Fonction trouver_min_sim(liste dissims_list):
    min_sim <- dissims_liste[0]
    pour chaque dissim dans dissims_liste:
        si dissim < min_sim alors
            min_sim <- dissim
    retourner min_sim

```

##sim_cosinus()

```
Fonction sim_cosinus(dict vect1, dict vect2): float {
    proscalaire <- 0
    norme1 <- 0
    norme2 <- 0

    Pour chaque cle dans vect1:
        Si cle est dans vect2 alors:
            proscalaire += vect1[cle] * vect2[cle]  
            norme1 += vect1[cle] ** 2  
            norme2 += vect2[cle] ** 2

    normes <- racine_carree(norme1) * racine_carree(norme2)

    Si normes == 0 alors:
        Retourner 0
    Sinon:
        cosinus <- proscalaire / normes
        Retourner cosinus
}

```
##_dissim()
```
Fonction _dissim(liste cluster1, liste cluster2) -> float: {

    dissim_total <- 0

    Pour chaque cluster_vect1 dans cluster1:
        Pour chaque cluster_vect2 dans cluster2:
            dissimilarity <- 1 - sim_cosinus(cluster_vect1, cluster_vect2)
            dissim_total += dissimilarity

    dissimilarity <- dissim_total / longueur de cluster1 * longueur de cluster2

    Retourner dissimilarity
}
```

#Version 2 : Classify simplifiée
Nous avons développé cette fonction classify utilisant le principe de deepcopy, cependant nous avons eu du mal à implémenter cette fonction lors de la fusion de nos codes c'est pourquoi il existe une deuxième version du code.

```


Fonction classify(self, entier n, float min_sim): list {
    new_cluster <- []
    list_vecteurs <- []
    pour chaque entry dans self.data:
        list_vecteurs.append([entry["vecteur"]])

dissim_dict <- {}  # Dictionnaire liant la dissimilarité de deux clusters à leurs indices dans la liste
dissims_list <- []  # Liste stockant toutes les dissimilarités

tant que la longueur de list_vecteurs > à n:
    Pour i de 0 à la longueur de list_vecteurs - 1:
        Pour j de i+1 à la longueur de list_vecteurs - 1:
                if i < len(list_vecteurs) and j < len(list_vecteurs):
                        dissim = CAH.calcul_dissim(list_vecteurs[i], list_vecteurs[j])
                        dissim_dict[dissim] <- (i, j)
                        dissims_list.append(dissim)

    #Trouver la dissimilarité minimale dans dissims_list
    min_sim <- self.find_min_sim(dissims_list)
    indices <- dissim_dict[min_sim]

    # Créer un nouveau cluster avec les vecteurs ayant les indices minimaux
    copy1 <- copy.deepcopy(list_vecteurs[indices[0]])
    copy2 <- copy.deepcopy(list_vecteurs[indices[1]])
    new_cluster.append(copy1)
    new_cluster.append(copy2)

    # Supprimer les vecteurs fusionnés de list_vecteurs et ajouter le nouveau cluster
    del list_vecteurs[max(indices)]
    del list_vecteurs[min(indices)]
    list_vecteurs.append(new_cluster)
  
retourner list_vecteurs
}
```

#Calcul du TF-IDF
```
Fonction tf_idf(self) -> dict: {
    result <- {}

    #Nb de txt total
    nb_total_txt <- longueur de self.data

    #dico pour stocker le nombre de textes contenant chaque mot
    nb_text_word <- {}

    Pour chaque texte dans self.data:
        text <- liste(item.values())[0]
        vecteur <- fonction CAH.text_2_vec(text, self.stoplist)

        #nb total de mots dans le texte
        nb_total_word <- Somme des valeurs du vecteur

        Pour chaque mot, freq dans vecteur:
            # Calcul du score TF
            tf <- freq / nb_total_word

            # Màj nb texte contenant le mot
            Si le mot in nb_text_word:
                nb_text_word[mot] <- nb_text_word[mot] + 1
            Sinon:
                nb_text_word[mot] <- 1

            # Calcul du IDF pour le mot
            idf <- log(nb_total_txt / nb_text_word[mot])

            # Calcul du TF-IDF pour ce mot
            tf_idf <- tf * idf

            result[mot] <- {"TF": tf, "IDF": idf, "TF_IDF": tf_idf}

    Retourner result
}
```
"""